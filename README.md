# Pinterest Data Pipeline 

## Contents
1. [Description](#description)
2. [Installation instructions](#installation-instructions)
3. [Usage instructions and setup](#usage-instructions-and-setup)
    1. [Configure the EC2 Kafka client](#configure-the-ec2-kafka-client)
    2. [Connect a MSK cluster to a S3 bucket](#connect-a-msk-cluster-to-a-s3-bucket)
    3. [Configuring an API in API Gateway](#configuring-an-api-in-api-gateway)
    4. [Databricks](#databricks)
    5. [Spark on Databricks](#spark-on-databricks)
    6. [AWS MWAA](#aws-mwaa)
    7. [Stream Processing: AWS Kinesis](#stream-processing-aws-kinesis)



## Description 
This project aims to create a system similar to the one used by Pinterest to crunch billions of data points every day to decide how to provide more value to their users. Specifically, the system here aims to extract, load and transform the data including cleaning and quereying of the data. 



## Installation instructions

Resources required for this project:

* [Apache Kafka](https://kafka.apache.org/documentation/)
* [Confluent Rest Proxy](https://docs.confluent.io/platform/current/kafka-rest/index.html)
* [Apache Spark](https://spark.apache.org/docs/latest/)
* [Databricks](https://docs.databricks.com/en/index.html)
* [PySpark](https://spark.apache.org/docs/latest/api/python/index.html)
* [AWS MSK](https://docs.aws.amazon.com/msk/latest/developerguide/what-is-msk.html)
* [AWS MSK Connect](https://docs.aws.amazon.com/msk/latest/developerguide/msk-connect.html)
* [AWS API Gateway](https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html)
* [Amazon Managed Workflows for Apache Airflow](https://docs.aws.amazon.com/mwaa/latest/userguide/what-is-mwaa.html)
* [Amazon Kinesis Streams](https://docs.aws.amazon.com/streams/latest/dev/introduction.html)


## Usage instructions and setup

Data used for this project was produced using the [user_posting_emulation.py](user_posting_emulation.py) script. This script connects to an AWS RDS database using an SQLAlchemy engine. The data is meant to replicate the type of data generated by Pinterest thus containing three different tables containing: pin data, geolocation data and user data. 

### Configure the EC2 Kafka client

- Firstly, a .pem key file needs to be created locally which contains the key pair associated with your EC2 instance. 
- In order to connect to the EC2 instance, you must run this code in your terminal: <u>ssh -i "key-pair.pem" ec2-user@your-ec2-instance-pubic-DNS</u>. 
- Kafka must then be installed on the EC2 client. In order to do this, an IAM authenticated MSK cluster is needed, which I was provided with in this project. The security rules for the EC2 instance to allow communication with the MSK cluster must also be set up. The version of Kafka has to match the one the cluster is running on. 
- Install the IAM MSK authentication package on your client EC2 machine. This package is necessary to connect to MSK clusters that require IAM authentication.
- Create an IAM access role that allows access to the MSK cluster.
- Configure the Kafka client in order to use AWS IAM authentication to the MSK cluster. This is done by modifying the <u>client.properties</u> file found in the "kafka_folder/bin" directory.
- Next, Kafka topics must be created. This is done by first retrieving and taking note of the Bootstrap servers string and the Plaintext Apache Zookeeper connection string which can be found using the MSK Management Console. The topics can be made using this command:  <u>./kafka-topics.sh --create --zookeeper "your_bootstrap_server_string" --replication-factor 2 --partitions 1 --topic your_topic_name</u> 

### Connect a MSK cluster to a S3 bucket

- In this project, an S3 bucket has been provided, but this needs to be created. Additionally, an IAM role that allows you to write to this bucket or a VPC Endpoint to S3 also need to be setup. 
- On the EC2 client, download the Confluent.io Amazon S3 Connector and copy it to your S3 bucket. 
- Next, create your custom plugin in the MSK connect console. 
- Create the connector with MSK Connect making sure that the topics.regex field in the connector configuration has the following structure: <u>your_UserId.*</u> This will ensure that data going through all the three previously created Kafka topics will get saved to your S3 bucket.

### Configuring an API in API Gateway

- The API used in this project was provided but this needs to be setup. 
- Next, a resource that allows you to build a PROXY integration for your API needs to be created. 
- For this resource create a HTTP ANY method. For the Endoint URL, make sure to copy the correct PublicDNS, from the EC2 machine you have been working on so far. 
- Deploy the API and make a note of the Invoke URL as this will be needed later. 
- This Kafka REST Proxy then needs to be set up on your EC2 client machine. Install the Confluent package for the Kafka REST Proxy on your EC2 client machine. Then allow the REST proxy to perform IAM authentication to the MSK cluster by modifying the <u>kafka-rest.properties</u>file.
- In order to run the REST proxy inside your EC2 client, use the following code: <u>./kafka-rest-start /home/ec2-user/confluent-7.2.0/etc/kafka-rest/kafka-rest.properties</u>.
- Finally, modify the [user_posting_emulation.py](user_posting_emulation.py) to send data to your Kafka topics using your API Invoke URL. You should send data from the three tables to their corresponding Kafka topic. You can check if the data is being stored in the S3 bucket. 

### Databricks

- In order to clean and query the data, you will need to read the data from your S3 bucket into Databricks.
- First, mount your S3 bucket to Databricks. The code for this can be found within the first cell of the Pinterest Data Cleaning and Queries.py notebook. 
- When reading in the data from the S3 bucket, you should create three different DataFrames containing the three different types of data mentioned earlier.

### Spark on Databricks

- Here, the data read in from the S3 bucket is cleaned and queried using the code found in the Pinterest Data Cleaning and Queries.py notebook.

### AWS MWAA

- A MWAA environment is needed for this part which was provided in this project. You will also be required to create an API token in Databricks to connect to your AWS account, as well as set up the MWAA-Databricks connection and create the requirements.txt file.
- Next, create and upload a DAG file into your MWAA environment like the one in [0eeeb621168f_dag.py](0eeeb621168f_dag.py).
- Then manually trigger the DAG to run the Databricks notebook that contains the code to clean and query your data.

### Stream Processing: AWS Kinesis

- Using Kinesis Data Streams create three different data streams corresponding to your three tables. 
- Configure your previously created REST API to allow it to invoke Kinesis actions. The API should be able to invoke the following actions: list streams in Kinesis, create, describe and delete streams, add records to streams.
- The [user_posting_emulation_streaming.py](user_posting_emulation_streaming.py) sends requests to the API, which adds one record at a time to the streams.
- A notebook like the Kinesis Data Stream.py notebook then needs to be created which will read the data from the streams, clean and query the data and finally writes the transformed data to Delta Tables in Databricks. 

